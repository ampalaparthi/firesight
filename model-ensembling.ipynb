{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a709407d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.8.1+cu111\n",
      "Torchvision Version:  0.9.1+cu111\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9fb8b389",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_path = \"saved-models/resnet/resnet.pth\"\n",
    "densenet_path = \"saved-models/densenet/densenet.pth\"\n",
    "vit_dir = \"saved-models/vit/flame-dataset-vit-1\"\n",
    "model_paths = [resnet_path, densenet_path, vit_dir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "c61310d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "use_pretrained = True\n",
    "resnet_model = models.resnet18(pretrained=use_pretrained).to(device)\n",
    "num_ftrs = resnet_model.fc.in_features\n",
    "resnet_model.fc = nn.Linear(num_ftrs, num_classes).to(device)\n",
    "\n",
    "resnet_model_features = models.resnet18(pretrained=use_pretrained).to(device)\n",
    "resnet_model_features.fc = nn.Identity()\n",
    "num_resnet_ftrs = num_ftrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "efc071a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet_model = models.densenet121(pretrained=use_pretrained).to(device)\n",
    "num_ftrs = densenet_model.classifier.in_features\n",
    "densenet_model.classifier = nn.Linear(num_ftrs, num_classes).to(device)\n",
    "\n",
    "densenet_model_features= models.densenet121(pretrained=use_pretrained).to(device)\n",
    "densenet_model_features.fc = nn.Identity()\n",
    "num_densenet_ftrs = num_ftrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "16093354",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "vit_model = ViTForImageClassification.from_pretrained(vit_dir)\n",
    "\n",
    "vit_model_features = copy.deepcopy(vit_model)\n",
    "vit_model_features.classifier = nn.Identity()\n",
    "\n",
    "num_vit_ftrs = vit_model.classifier.in_features\n",
    "\n",
    "class VitWrapper(nn.Module):\n",
    "    def __init__(self, huggingface_model):\n",
    "        super(VitWrapper, self).__init__()\n",
    "        self.huggingface_model = huggingface_model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.huggingface_model(x).logits.to(device) #grab logits\n",
    "\n",
    "vit_model = VitWrapper(vit_model)\n",
    "vit_model = vit_model.to(device)\n",
    "\n",
    "vit_model_features = VitWrapper(vit_model_features)\n",
    "vit_model_features = vit_model_features.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c36f69ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_model.load_state_dict(torch.load(resnet_path, map_location=device))\n",
    "densenet_model.load_state_dict(torch.load(densenet_path, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a34b9def",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f72f518478fd423da0fe9eb9eecb38dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/39375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb9a13920854fe9bedf405d95ca859b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/8617 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-320a3a01cff974fc\n",
      "Reusing dataset image_folder (/home/ubuntu/.cache/huggingface/datasets/image_folder/default-320a3a01cff974fc/0.0.0/48efdc62d40223daee675ca093d163bcb6cb0b7d7f93eb25aebf5edca72dc597)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f609f41696c642e3a81bce28730cb2ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 31500\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 7875\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 8617\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in data\n",
    "train_folder = \"../train-val/Training/\"\n",
    "test_folder = \"../test/Test/\"\n",
    "import torch.utils.data as data\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"imagefolder\", ignore_verifications=True, data_files={\"train\": f\"{train_folder}**\", \"test\": f\"{test_folder}**\"})\n",
    "import datasets\n",
    "train_valid = ds['train'].train_test_split(test_size=0.2)\n",
    "ds = datasets.DatasetDict({\n",
    "    'train': train_valid['train'],\n",
    "    'val': train_valid['test'],\n",
    "    'test': ds['test']\n",
    "})\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a51feb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied over from resnet-densenet\n",
    "model_name = \"resnet\"\n",
    "num_classes = 2\n",
    "batch_size = 8\n",
    "num_epochs = 15\n",
    "feature_extract = True\n",
    "data_dir = \"train-val/Training\"\n",
    "test_dir = \"test/Test\"\n",
    "percent_val = 0.1\n",
    "save_path = \"saved-models\"\n",
    "input_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "aa8131ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test':transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'vit_train': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    ]),\n",
    "    'vit_val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    ]),\n",
    "    'vit_test':transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    ]),\n",
    "    'torchvision_train': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'torchvision_val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'torchvision_test':transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "21c62ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "image_datasets = {}\n",
    "orig_set = datasets.ImageFolder(train_folder)\n",
    "n = len(orig_set)\n",
    "n_val = int(percent_val * n)\n",
    "train_dataset, val_dataset = random_split(orig_set, [n-n_val, n_val])\n",
    "train_dataset_vit, val_dataset_vit = random_split(orig_set, [n-n_val, n_val])\n",
    "train_dataset_torchvision, val_dataset_torchvision = random_split(orig_set, [n-n_val, n_val])\n",
    "val_dataset.dataset.transform = data_transforms[\"val\"]\n",
    "train_dataset.dataset.transform = data_transforms[\"train\"]\n",
    "val_dataset_vit.dataset.transform = data_transforms[\"vit_val\"]\n",
    "train_dataset_vit.dataset.transform = data_transforms[\"vit_train\"]\n",
    "val_dataset_vit.dataset.transform = data_transforms[\"torchvision_val\"]\n",
    "train_dataset_vit.dataset.transform = data_transforms[\"torchvision_train\"]\n",
    "image_datasets[\"val\"] = val_dataset\n",
    "image_datasets[\"train\"] = train_dataset\n",
    "image_datasets[\"test\"] = datasets.ImageFolder(test_folder, data_transforms[\"test\"])\n",
    "image_datasets[\"vit_val\"] = val_dataset_vit\n",
    "image_datasets[\"vit_train\"] = train_dataset_vit\n",
    "image_datasets[\"vit_test\"] = datasets.ImageFolder(test_folder, data_transforms[\"vit_test\"])\n",
    "image_datasets[\"torchvision_val\"] = val_dataset_torchvision\n",
    "image_datasets[\"torchvision_train\"] = train_dataset_torchvision\n",
    "image_datasets[\"torchvision_test\"] = datasets.ImageFolder(test_folder, data_transforms[\"torchvision_test\"])\n",
    "\n",
    "# Create training, validation, and test dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val', 'test', 'vit_train', 'vit_val', 'vit_test', 'torchvision_train', 'torchvision_val', 'torchvision_test']}\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "1c24786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VotingEnsemble(nn.Module):\n",
    "    def __init__(self, model_list):\n",
    "        super(VotingEnsemble, self).__init__()\n",
    "        self.model_list = model_list\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # normalize according to the type of model\n",
    "        results_list = []\n",
    "        for model_i in self.model_list:\n",
    "            # normalize according to the type of model\n",
    "            if model_i.__class__.__name__ == \"VitWrapper\":\n",
    "                normalize_transform = transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "            else:\n",
    "                normalize_transform = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            x_norm = normalize_transform(x.clone()).to(device)\n",
    "            scores = model_i(x_norm)\n",
    "            _, preds = scores.max(1)\n",
    "            results_list.append(preds)\n",
    "        combined_results = torch.stack(results_list)\n",
    "        voted_pred = torch.mode(combined_results, dim=0)[0]\n",
    "        return torch.mode(combined_results, dim=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "e9113020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check accuracy on test set\n",
    "voting_ensemble = VotingEnsemble([resnet_model, densenet_model, vit_model])\n",
    "def test_ensemble_accuracy(loader, ensemble, is_val):\n",
    "    if is_val:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    ensemble.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            preds = ensemble(x)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "eaa99048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on validation set\n",
      "Got 3878 / 3937 correct (98.50)\n",
      "Checking accuracy on test set\n",
      "Got 6484 / 8617 correct (75.25)\n"
     ]
    }
   ],
   "source": [
    "test_ensemble_accuracy(dataloaders_dict[\"val\"], voting_ensemble, True)\n",
    "test_ensemble_accuracy(dataloaders_dict[\"test\"], voting_ensemble, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "0762cbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(loader, model, is_val):\n",
    "    if is_val:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "02f0dd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on test set\n",
      "Got 6743 / 8617 correct (78.25)\n",
      "Checking accuracy on test set\n",
      "Got 6275 / 8617 correct (72.82)\n",
      "Checking accuracy on test set\n",
      "Got 6126 / 8617 correct (71.09)\n"
     ]
    }
   ],
   "source": [
    "test_accuracy(dataloaders_dict[\"vit_test\"], vit_model, False)\n",
    "test_accuracy(dataloaders_dict[\"torchvision_test\"], resnet_model, False)\n",
    "test_accuracy(dataloaders_dict[\"torchvision_test\"], densenet_model, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "71ee2c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatEnsemble(nn.Module):\n",
    "    def __init__(self, model_list, num_features, num_classes):\n",
    "        super(ConcatEnsemble, self).__init__()\n",
    "        self.model_list = model_list\n",
    "        self.num_features = num_features\n",
    "        self.classifier = nn.Linear(num_features, num_classes).to(device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # normalize according to the type of model\n",
    "        scores_list = []\n",
    "        for model_i in self.model_list:\n",
    "            # normalize according to the type of model\n",
    "            if model_i.__class__.__name__ == \"VitWrapper\":\n",
    "                normalize_transform = transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "            else:\n",
    "                normalize_transform = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            x_norm = normalize_transform(x.clone()).to(device)\n",
    "            \n",
    "            scores = model_i(x_norm)\n",
    "            scores_list.append(scores)\n",
    "        combined_results = torch.cat(scores_list, dim=1)\n",
    "        out = self.classifier(torch.nn.functional.relu(combined_results))\n",
    "        \n",
    "        #_, preds = out.max(1)\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "a045955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in resnet_model_features.parameters():\n",
    "    param.requires_grad = (False)\n",
    "for param in densenet_model_features.parameters():\n",
    "    param.requires_grad = (False)\n",
    "for param in vit_model_features.parameters():\n",
    "    param.requires_grad = (False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "0e2d9ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_ensemble = ConcatEnsemble([resnet_model_features, densenet_model_features, vit_model_features], 2280, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "01176c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61902247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/14\n",
      "----------\n",
      "train Loss: 0.0083 Acc: 0.9970\n",
      "val Loss: 0.0112 Acc: 0.9949\n",
      "\n",
      "Epoch 1/14\n",
      "----------\n",
      "train Loss: 0.0081 Acc: 0.9975\n",
      "val Loss: 0.0187 Acc: 0.9936\n",
      "\n",
      "Epoch 2/14\n",
      "----------\n",
      "train Loss: 0.0069 Acc: 0.9979\n",
      "val Loss: 0.0076 Acc: 0.9977\n",
      "\n",
      "Epoch 3/14\n",
      "----------\n",
      "train Loss: 0.0062 Acc: 0.9981\n",
      "val Loss: 0.0100 Acc: 0.9957\n",
      "\n",
      "Epoch 4/14\n",
      "----------\n",
      "train Loss: 0.0070 Acc: 0.9977\n",
      "val Loss: 0.0067 Acc: 0.9977\n",
      "\n",
      "Epoch 5/14\n",
      "----------\n",
      "train Loss: 0.0059 Acc: 0.9980\n",
      "val Loss: 0.0073 Acc: 0.9970\n",
      "\n",
      "Epoch 6/14\n",
      "----------\n",
      "train Loss: 0.0057 Acc: 0.9983\n",
      "val Loss: 0.0057 Acc: 0.9980\n",
      "\n",
      "Epoch 7/14\n",
      "----------\n",
      "train Loss: 0.0054 Acc: 0.9984\n",
      "val Loss: 0.0102 Acc: 0.9970\n",
      "\n",
      "Epoch 8/14\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.9982\n",
      "val Loss: 0.0101 Acc: 0.9975\n",
      "\n",
      "Epoch 9/14\n",
      "----------\n",
      "train Loss: 0.0055 Acc: 0.9983\n",
      "val Loss: 0.0115 Acc: 0.9959\n",
      "\n",
      "Epoch 10/14\n",
      "----------\n",
      "train Loss: 0.0049 Acc: 0.9984\n",
      "val Loss: 0.0062 Acc: 0.9975\n",
      "\n",
      "Epoch 11/14\n",
      "----------\n",
      "train Loss: 0.0048 Acc: 0.9986\n",
      "val Loss: 0.0090 Acc: 0.9972\n",
      "\n",
      "Epoch 12/14\n",
      "----------\n",
      "train Loss: 0.0056 Acc: 0.9982\n",
      "val Loss: 0.0096 Acc: 0.9967\n",
      "\n",
      "Epoch 13/14\n",
      "----------\n",
      "train Loss: 0.0053 Acc: 0.9984\n"
     ]
    }
   ],
   "source": [
    "# Setup the loss fxn\n",
    "optimizer_ft = optim.SGD(concat_ensemble.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs = 15\n",
    "# Train and evaluate\n",
    "concat_ensemble, hist = train_model(concat_ensemble, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))\n",
    "torch.save(concat_ensemble.state_dict(), os.path.join(os.path.join(save_path, \"concat-ensemble\"), \"concat-ensemble.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "5f7f3db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on validation set\n",
      "Got 3922 / 3937 correct (99.62)\n",
      "Checking accuracy on test set\n",
      "Got 6151 / 8617 correct (71.38)\n"
     ]
    }
   ],
   "source": [
    "test_accuracy(dataloaders_dict[\"val\"], concat_ensemble, True)\n",
    "test_accuracy(dataloaders_dict[\"test\"], concat_ensemble, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6af2be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p37)",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
